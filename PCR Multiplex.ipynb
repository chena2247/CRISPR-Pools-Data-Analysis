{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reads in a folder of data and creates a frequency table for primer pairs that match the reads\n",
    "## Basic, only for separate reads fastq documents\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import MeltingTemp as mt\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna, generic_protein\n",
    "import sys\n",
    "import os\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "from Bio.Seq import Seq\n",
    "import subprocess\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "SeqFilePath = '/Users/annechen/Downloads/CRISPR Pools Data/test2'\n",
    "PrimerID = '/Users/annechen/Downloads/sgRNAs for multiplexed CRISPR - Primer Sequences Only.csv'\n",
    "\n",
    "output_folder = join(SeqFilePath, 'testout')\n",
    "## Attempting to put data in separate output_folder\n",
    "if not os.path.isdir(output_folder): \n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "def primerfreq(SeqFile, start_index):\n",
    "    \n",
    "    ############################################################################\n",
    "    ## Load CSV File of Primer IDs into pandas dataframe\n",
    "    \n",
    "    primer = pd.read_csv(PrimerID)\n",
    "    primer['Frequency'] = 0\n",
    "    \n",
    "    ############################################################################\n",
    "    ## Make a dictionary for sequences\n",
    "    ## Read in the fastq file  \n",
    "\n",
    "    SeqDict = {}\n",
    "    freq = {}\n",
    "\n",
    "    with open(SeqFile, 'rU') as handle:\n",
    "        for record in SeqIO.parse(handle, \"fastq\"):\n",
    "            SeqDict[str(record.id)] = str(record.seq)\n",
    "            #ProbeseqList.append(str(record.id)) ## Also make a list of all of the probe sequences for writing the output file.\n",
    "\n",
    "    ############################################################################\n",
    "    ##  Make Dictionary by looping through list and count the frequencies\n",
    "\n",
    "    for record in SeqDict:\n",
    "        seq = str(SeqDict[record])\n",
    "        added = False\n",
    "        newcode = ''\n",
    "        if seq[-8:] != 'GGGGGGGG':\n",
    "            for i in freq:\n",
    "                if i[0:15] == seq[0:15]:\n",
    "                    freq[i] = freq[i] + 1\n",
    "                    added = True\n",
    "                else:\n",
    "                    newcode = seq[0:15]\n",
    "            if added == False:\n",
    "                freq[newcode] = 1\n",
    "\n",
    "    ############################################################################\n",
    "    ##  Convert previous dictionary to dataframe \n",
    "\n",
    "    fTable = pd.DataFrame.from_dict(freq, orient='index', columns=['Frequency'])\n",
    "    fTable = fTable.sort_values('Frequency',ascending=False)\n",
    "    fTable['ID'] = '-'\n",
    "    \n",
    "    ############################################################################\n",
    "    ##  Enter matching primer ID if exists on fTable\n",
    "    ##  Enter frequency of appearance on primer dataframe\n",
    "\n",
    "    for seq_index, seq_row in fTable.iterrows():\n",
    "        for prim_index, prim_row in primer.iterrows():\n",
    "            primerID = primer.iloc[prim_index,1]\n",
    "            #print seq_index + ' ' + primerID[start_index+0:start_index+15]\n",
    "            if seq_index == primerID[start_index:start_index+15]:\n",
    "                fTable.at[seq_index,'ID'] = primer.iloc[prim_index,0]\n",
    "                primer.iloc[prim_index,2] = fTable.at[seq_index,'Frequency']\n",
    "    ##pd.set_option('display.max_rows', 208) ##allows you to view entirety of fTable w/o truncation\n",
    "\n",
    "    ##print (fTable)\n",
    "    \n",
    "    primer = primer.sort_values('Frequency',ascending=False)\n",
    "    primer = primer.reset_index()\n",
    "    del primer['index']\n",
    "    \n",
    "    ############################################################################\n",
    "    ##  Export primer frequency data as csv, managing file path\n",
    "    \n",
    "    csv_file = join (output_folder, os.path.splitext(os.path.basename(SeqFile))[0] + '_primerMatch.csv')\n",
    "    \n",
    "    ##csv_file = join(output_folder, os.path.splitext(SeqFile)[0] + '_primerMatch.csv')\n",
    "\n",
    "    primer.to_csv(csv_file, sep='\\t', index=False)\n",
    "    primercsv = pd.read_csv('primerMatch.csv',sep='\\t')\n",
    "    #print(primercsv)\n",
    "\n",
    "\n",
    "for f in listdir(SeqFilePath):\n",
    "    if not f.startswith('.'):\n",
    "        ## In R2 file, sequencing left off first base, readjusted index to shift one right\n",
    "        if '_R2_' in f:\n",
    "            ##primerfreq(f,1)\n",
    "            primerfreq(join(SeqFilePath,f),1)\n",
    "        if '_R1_' in f:\n",
    "            ##primerfreq(f,0)\n",
    "\n",
    "            primerfreq(join(SeqFilePath, f),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1_S2_L001_001.fastq\n",
      "1-1_S2_L001_001.fastq\n",
      "36_S31_L001_001.fastq\n",
      "36_S31_L001_001.fastq\n"
     ]
    }
   ],
   "source": [
    "## Code for csv of paired primers from a list of seperate reads\n",
    "\n",
    "sep_reads = '/Users/annechen/Downloads/CRISPR Pools Data/test'\n",
    "\n",
    "############################################################################\n",
    "## Finding matching files in the folder to designate R1 as forward and R2 as reverse\n",
    "\n",
    "fastq_data = []\n",
    "\n",
    "for f1 in listdir(sep_reads):\n",
    "    if '_R1_' in f1 and not f1.startswith('.'):\n",
    "        forward_f = f1\n",
    "        for f2 in listdir(sep_reads):\n",
    "            if '_R2_' in f2 and not f2.startswith('.'):\n",
    "                if f1.replace('_R1','') == f2.replace('_R2',''):\n",
    "                    reverse_f = f2\n",
    "                    \n",
    "                    ############################################################################\n",
    "                    ## Creating dataframe with ID, forward, and reverse reads\n",
    "                    \n",
    "                    with open (join(sep_reads,f1), 'rU') as forward:\n",
    "                        for forward_data in SeqIO.parse(forward, 'fastq'):\n",
    "                            fastq_data.append([str(forward_data.id), str(forward_data.seq)])\n",
    "                    \n",
    "                    fastq_df = pd.DataFrame(fastq_data, columns = ['read_ID', 'forward'])\n",
    "                    index_pos = 0\n",
    "                    \n",
    "                    fastq_df['reverse'] = ''\n",
    "                    with open (join(sep_reads,f2),'rU') as reverse:\n",
    "                        for reverse_data in SeqIO.parse(reverse, 'fastq'):\n",
    "                            rev_code = str(reverse_data.seq)\n",
    "                            ##print fastq_df.at[index_pos, 'read_ID']\n",
    "                            if str(reverse_data.id) == fastq_df.at[index_pos,'read_ID']:\n",
    "                                fastq_df.at[index_pos, 'reverse'] = rev_code\n",
    "                                index_pos += 1\n",
    "                                \n",
    "                    ############################################################################            \n",
    "                    ## Making file name            \n",
    "                    initial_file_name = os.path.basename(f1.replace('_R1',''))\n",
    "                    combined_file = join(sep_reads, os.path.splitext(initial_file_name)[0] + '_combined.csv')\n",
    "                    \n",
    "                    ##print (initial_file_name)\n",
    "                    ##print os.path.basename(initial_file_name)\n",
    "                    ##os.path.splitext(os.path.basename(SeqFile))[0] \n",
    "                    ## Making csv file\n",
    "                    fastq_df.to_csv(combined_file, sep='\\t', index=False)\n",
    "                    ##fastqcsv = pd.read_csv()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working doc that reads in a folder of combined reads and outputs data (only works on one doc at a time)\n",
    "\n",
    "PrimerID = '/Users/annechen/Downloads/sgRNAs for multiplexed CRISPR - Primer Sequences Only.csv'\n",
    "combined = '/Users/annechen/Downloads/CRISPR Pools Data/test2/results/1-1_S2_L001_001_combined.csv'\n",
    "path = '/Users/annechen/Downloads/CRISPR Pools Data/test2/results'\n",
    "\n",
    "\n",
    "comb_reads = pd.read_csv(combined, sep='\\t')\n",
    "primer = pd.read_csv(PrimerID)\n",
    "primer.head()\n",
    "\n",
    "############################################################################\n",
    "## Create 6 dataframes to hold different information\n",
    "\n",
    "concordant = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "discordant = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "onematch = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "nomatch = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "freqTable = pd.DataFrame(columns = ['Forward Prim ID', 'Reverse Prim ID', 'Frequency'])\n",
    "aggData = {}\n",
    "\n",
    "############################################################################\n",
    "## Load PrimerID values into freqTable\n",
    "\n",
    "## Need to change this part of code according to name of primer sequences\n",
    "for index, row in primer.iterrows():\n",
    "    if index == 30:\n",
    "        break\n",
    "    freqTable = freqTable.append({'Forward Prim ID':primer.iloc[index,0],'Reverse Prim ID':primer.iloc[index+30, 0], 'Frequency':0}, ignore_index=True)\n",
    "                         \n",
    "############################################################################\n",
    "## Iterate through combined list to sort read pairs\n",
    "\n",
    "for seq_index, seq_row in comb_reads.iterrows():\n",
    "    \n",
    "    fseq = comb_reads.iloc[seq_index,1]\n",
    "    rseq = comb_reads.iloc[seq_index,2]\n",
    "    \n",
    "    ##Gfor and Grev = True if there is G-Tail at end of sequence, False if there is no G-tail\n",
    "    Gfor = fseq[-8:] == 'GGGGGGGG'\n",
    "    Grev = rseq[-8:] == 'GGGGGGGG'\n",
    "    ##fID and rID will be changed later if a match is found and there is no G-tail\n",
    "    fID = 'no forward match'\n",
    "    rID = 'no reverse match'\n",
    "    \n",
    "    ############################################################################\n",
    "    ## Match each read without a G-tail to a read ID\n",
    "    \n",
    "    if not Gfor or not Grev:\n",
    "        for p_index, p_row in primer.iterrows():\n",
    "            primerID = primer.iloc[p_index,1]\n",
    "\n",
    "            if not Gfor and fseq[0:15] == primerID[0:15]:\n",
    "                fID = primer.iloc[p_index, 0]\n",
    "            if not Grev and rseq[0:15] == primerID[1:16]:\n",
    "                rIDcode = primer.iloc[p_index, 0]\n",
    "                ## Need to change this part of code according to name of primer sequences (manually matches rev code to forward by subtracting 30)\n",
    "                rID = rIDcode[0:2] + str(int(rIDcode[2:])-30)\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    ## Sort read into one of four categories depending on presence of G-Tail and PrimerID Match\n",
    "    \n",
    "    if Gfor and Grev:\n",
    "        nomatch = pd.concat([nomatch, comb_reads.iloc[[seq_index]]])\n",
    "        \n",
    "    elif fID == rID:\n",
    "        ## print 'concordant ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "        concordant = pd.concat([concordant, comb_reads.iloc[[seq_index]]])\n",
    "        \n",
    "        ## Input info into concordant pair frequency table\n",
    "        for index, row in freqTable.iterrows():\n",
    "            if fID == freqTable.iloc[index,0]:\n",
    "                freqTable.iloc[index,2] += 1\n",
    "        \n",
    "    elif fID != 'no forward match' and rID != 'no reverse match':\n",
    "        ## print 'discordant ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "        discordant = pd.concat([discordant, comb_reads.iloc[[seq_index]]])\n",
    "        \n",
    "    elif fID != 'no forward match' or rID != 'no reverse match':\n",
    "        onematch = pd.concat([onematch, comb_reads.iloc[[seq_index]]])\n",
    "        ## print 'one match ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "        \n",
    "    else:\n",
    "        nomatch = pd.concat([nomatch, comb_reads.iloc[[seq_index]]])\n",
    "\n",
    "## Sort freqTable for primer freq from largest to smallest\n",
    "freqTable = freqTable.sort_values('Frequency', ascending=False)\n",
    "freqTable = freqTable.reset_index()\n",
    "del freqTable['index']\n",
    "\n",
    "############################################################################\n",
    "## Creating aggregate data dictionary and load into new dataframe\n",
    "\n",
    "aggData['Concordant'] = len(concordant.index)\n",
    "aggData['Discordant'] = len(discordant.index)\n",
    "aggData['One ID Match'] = len(onematch.index)\n",
    "aggData['No ID Match'] = len(nomatch.index)\n",
    "\n",
    "aggData_df = pd.DataFrame(aggData.items(), columns=['Pair Type', 'Frequency'])\n",
    "\n",
    "############################################################################\n",
    "## Create csv files for each dataframe\n",
    "\n",
    "concord_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_concordant_file.csv')\n",
    "concordant.to_csv(concord_csv, sep='\\t', index=False)\n",
    "\n",
    "discord_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_discordant_file.csv')\n",
    "discordant.to_csv(discord_csv, sep='\\t', index=False)\n",
    "\n",
    "onematch_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_single_match_file.csv')\n",
    "onematch.to_csv(onematch_csv, sep='\\t', index=False)\n",
    "\n",
    "nomatch_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_no_matches_file.csv')\n",
    "nomatch.to_csv(nomatch_csv, sep='\\t', index=False)\n",
    "\n",
    "freqTable_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_concordant_freqTable.csv')\n",
    "freqTable.to_csv(freqTable_csv, sep='\\t', index=False)\n",
    "\n",
    "aggData_df_csv = join(path, os.path.splitext(os.path.basename(combined))[0] + '_aggregate_data.csv')\n",
    "aggData_df.to_csv(aggData_df_csv, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reads in a folder of seperate fastq reads\n",
    "## Creates output of combined reads from fastq files and 6 docs of data:\n",
    "## 6 docs: list of concordant, discordant, single match, and no match reads, a frequency table of concordant reads, and an aggregate table of the number of reads of each type\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import MeltingTemp as mt\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna, generic_protein\n",
    "import sys\n",
    "import os\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "from Bio.Seq import Seq\n",
    "import subprocess\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "sep_reads = '/Users/annechen/Downloads/CRISPR Pools Data/test2'\n",
    "PrimerID = '/Users/annechen/Downloads/sgRNAs for multiplexed CRISPR - Primer Sequences Only.csv'\n",
    "\n",
    "############################################################################\n",
    "## Creating output folder to put results in\n",
    "\n",
    "output_folder = join(sep_reads, 'results')\n",
    "## Put data in separate output_folder\n",
    "if not os.path.isdir(output_folder): \n",
    "    os.mkdir(output_folder)\n",
    "    \n",
    "############################################################################\n",
    "## Create function to analyze combined reads csv file and split data into the 6 docs\n",
    "                    \n",
    "def pairAnalyze(CombSeqFile, PrimerIDFile):\n",
    "    \n",
    "    comb_reads = pd.read_csv(CombSeqFile, sep='\\t')\n",
    "    primer = pd.read_csv(PrimerIDFile)\n",
    "    \n",
    "    ############################################################################\n",
    "    ## Create 6 dataframes to hold different information\n",
    "\n",
    "    concordant = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "    discordant = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "    onematch = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "    nomatch = pd.DataFrame(columns = ['read_ID', 'forward', 'reverse'])\n",
    "    freqTable = pd.DataFrame(columns = ['Forward Prim ID', 'Reverse Prim ID', 'Frequency'])\n",
    "    aggData = {}\n",
    "    \n",
    "    ############################################################################\n",
    "    ## Load PrimerID values into freqTable\n",
    "\n",
    "    ## Need to change this part of code according to name of primer sequences\n",
    "    for index, row in primer.iterrows():\n",
    "        if index == 30:\n",
    "            break\n",
    "        freqTable = freqTable.append({'Forward Prim ID':primer.iloc[index,0],'Reverse Prim ID':primer.iloc[index+30, 0], 'Frequency':0}, ignore_index=True)\n",
    "\n",
    "    ############################################################################\n",
    "    ## Iterate through combined list to sort read pairs\n",
    "\n",
    "    for seq_index, seq_row in comb_reads.iterrows():\n",
    "\n",
    "        fseq = comb_reads.iloc[seq_index,1]\n",
    "        rseq = comb_reads.iloc[seq_index,2]\n",
    "\n",
    "        ##Gfor and Grev = True if there is G-Tail at end of sequence, False if there is no G-tail\n",
    "        Gfor = fseq[-8:] == 'GGGGGGGG'\n",
    "        Grev = rseq[-8:] == 'GGGGGGGG'\n",
    "        ##fID and rID will be changed later if a match is found and there is no G-tail\n",
    "        fID = 'no forward match'\n",
    "        rID = 'no reverse match'\n",
    "        \n",
    "        ############################################################################\n",
    "        ## Match each read without a G-tail to a read ID\n",
    "\n",
    "        if not Gfor or not Grev:\n",
    "            for p_index, p_row in primer.iterrows():\n",
    "                primerID = primer.iloc[p_index,1]\n",
    "\n",
    "                if not Gfor and fseq[0:15] == primerID[0:15]:\n",
    "                    fID = primer.iloc[p_index, 0]\n",
    "                if not Grev and rseq[0:15] == primerID[1:16]:\n",
    "                    rIDcode = primer.iloc[p_index, 0]\n",
    "                    ## Need to change this part of code according to name of primer sequences (manually matches rev code to forward by subtracting 30)\n",
    "                    rID = rIDcode[0:2] + str(int(rIDcode[2:])-30)\n",
    "\n",
    "        ############################################################################\n",
    "        ## Sort read into one of four categories depending on presence of G-Tail and PrimerID Match\n",
    "\n",
    "        if Gfor and Grev:\n",
    "            nomatch = pd.concat([nomatch, comb_reads.iloc[[seq_index]]])\n",
    "\n",
    "        elif fID == rID:\n",
    "            ## print 'concordant ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "            concordant = pd.concat([concordant, comb_reads.iloc[[seq_index]]])\n",
    "\n",
    "            ## Input info into concordant pair frequency table\n",
    "            for index, row in freqTable.iterrows():\n",
    "                if fID == freqTable.iloc[index,0]:\n",
    "                    freqTable.iloc[index,2] += 1\n",
    "                \n",
    "        elif fID != 'no forward match' and rID != 'no reverse match':\n",
    "            ## print 'discordant ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "            discordant = pd.concat([discordant, comb_reads.iloc[[seq_index]]])\n",
    "\n",
    "        elif fID != 'no forward match' or rID != 'no reverse match':\n",
    "            onematch = pd.concat([onematch, comb_reads.iloc[[seq_index]]])\n",
    "            ## print 'one match ' + comb_reads.iloc[seq_index, 0][-4:]\n",
    "\n",
    "        else:\n",
    "            nomatch = pd.concat([nomatch, comb_reads.iloc[[seq_index]]])\n",
    "\n",
    "    ## Sort freqTable for primer freq from largest to smallest\n",
    "    freqTable = freqTable.sort_values('Frequency', ascending=False)\n",
    "    freqTable = freqTable.reset_index()\n",
    "    del freqTable['index']\n",
    "\n",
    "    ############################################################################\n",
    "    ## Creating aggregate data dictionary and load into new dataframe\n",
    "\n",
    "    aggData['Concordant'] = len(concordant.index)\n",
    "    aggData['Discordant'] = len(discordant.index)\n",
    "    aggData['One ID Match'] = len(onematch.index)\n",
    "    aggData['No ID Match'] = len(nomatch.index)\n",
    "\n",
    "    aggData_df = pd.DataFrame(aggData.items(), columns=['Pair Type', 'Frequency'])\n",
    "\n",
    "    ############################################################################\n",
    "    ## Create csv files for each dataframe\n",
    "\n",
    "    concord_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_concordant_file.csv')\n",
    "    concordant.to_csv(concord_csv, sep='\\t', index=False)\n",
    "\n",
    "    discord_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_discordant_file.csv')\n",
    "    discordant.to_csv(discord_csv, sep='\\t', index=False)\n",
    "\n",
    "    onematch_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_single_match_file.csv')\n",
    "    onematch.to_csv(onematch_csv, sep='\\t', index=False)\n",
    "\n",
    "    nomatch_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_no_matches_file.csv')\n",
    "    nomatch.to_csv(nomatch_csv, sep='\\t', index=False)\n",
    "\n",
    "    freqTable_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_concordant_freqTable.csv')\n",
    "    freqTable.to_csv(freqTable_csv, sep='\\t', index=False)\n",
    "\n",
    "    aggData_df_csv = join(os.path.dirname(CombSeqFile), os.path.splitext(os.path.basename(CombSeqFile))[0] + '_aggregate_data.csv')\n",
    "    aggData_df.to_csv(aggData_df_csv, sep='\\t', index=False)\n",
    "\n",
    "############################################################################\n",
    "## Fill results folder with combined reads pooling paired separate reads together\n",
    "\n",
    "for f1 in listdir(sep_reads):\n",
    "    if '_R1_' in f1 and not f1.startswith('.'):\n",
    "        forward_f = f1\n",
    "        for f2 in listdir(sep_reads):\n",
    "            if '_R2_' in f2 and not f2.startswith('.'):\n",
    "                if f1.replace('_R1','') == f2.replace('_R2',''):\n",
    "                    \n",
    "                    fastq_data = []\n",
    "                    reverse_f = f2\n",
    "                    \n",
    "                    ############################################################################\n",
    "                    ## Creating dataframe with ID, forward, and reverse reads\n",
    "                    \n",
    "                    with open (join(sep_reads,f1), 'rU') as forward:\n",
    "                        for forward_data in SeqIO.parse(forward, 'fastq'):\n",
    "                            fastq_data.append([str(forward_data.id), str(forward_data.seq)])\n",
    "                    \n",
    "                    fastq_df = pd.DataFrame(fastq_data, columns = ['read_ID', 'forward'])\n",
    "                    index_pos = 0\n",
    "                    \n",
    "                    fastq_df['reverse'] = ''\n",
    "                    with open (join(sep_reads,f2),'rU') as reverse:\n",
    "                        for reverse_data in SeqIO.parse(reverse, 'fastq'):\n",
    "                            rev_code = str(reverse_data.seq)\n",
    "                            ##print fastq_df.at[index_pos, 'read_ID']\n",
    "                            if str(reverse_data.id) == fastq_df.at[index_pos,'read_ID']:\n",
    "                                fastq_df.at[index_pos, 'reverse'] = rev_code\n",
    "                                index_pos += 1\n",
    "                                \n",
    "                    ############################################################################            \n",
    "                    ## Making file name            \n",
    "                    initial_file_name = os.path.basename(f1.replace('_R1',''))\n",
    "                    combined_file = join(output_folder, os.path.splitext(initial_file_name)[0] + '_combined.csv')\n",
    "                    \n",
    "                    ## Making csv file\n",
    "                    fastq_df.to_csv(combined_file, sep='\\t', index=False)\n",
    "\n",
    "## Load CSV File of Primer IDs into pandas dataframe\n",
    "##primer = pd.read_csv(PrimerID)\n",
    "\n",
    "for f in listdir(output_folder):\n",
    "    if not f.startswith('.'):\n",
    "        pairAnalyze(join(output_folder, f), PrimerID)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes a csv containing all the data necessary to graph the results\n",
    "## Requires a folder of all data files generated from previous cell\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "data = '/Users/annechen/Downloads/CRISPR Pools Data/results'\n",
    "PrimerID = '/Users/annechen/Downloads/sgRNAs for multiplexed CRISPR - Primer Sequences Only.csv'\n",
    "\n",
    "primer = pd.read_csv(PrimerID)\n",
    "graph_data = pd.DataFrame(columns = ['Sample ID', 'Total Reads', 'Discordant', 'Single match', 'No match', 'Concordant'])\n",
    "avg_rep = pd.DataFrame(columns = ['Forward ID', 'Reverse ID', 'Forward Sequence', 'Reverse Sequence', '% Frequency'])\n",
    "\n",
    "############################################################################\n",
    "## Input info from primer doc into final aggregate dataframes\n",
    "\n",
    "for index, row in primer.iterrows():\n",
    "    if index < 30:\n",
    "        primerID = primer.iloc[index,1]\n",
    "        pairs = primer.iloc[index,0] + ', ' + primer.iloc[index+30,0]\n",
    "        graph_data[pairs] = 0\n",
    "            \n",
    "        avg_rep = avg_rep.append({'Forward ID':primer.iloc[index,0],'Reverse ID':primer.iloc[index+30,0], \n",
    "                                  'Forward Sequence':primer.iloc[index,1], 'Reverse Sequence':primer.iloc[index+30,1]}, \n",
    "                                 ignore_index=True)\n",
    "\n",
    "\n",
    "for f in listdir(data):\n",
    "    \n",
    "    if '_aggregate_data.csv' in f:\n",
    "        agg_data = pd.read_csv(join(data, f), sep='\\t')\n",
    "        sample_name = f.split('_')[0]\n",
    "        total = agg_data.iloc[0,1] + agg_data.iloc[1,1] + agg_data.iloc[2,1] + agg_data.iloc[3,1]\n",
    "        graph_data = graph_data.append({'Sample ID':sample_name, 'Total Reads':total, 'Discordant':agg_data.iloc[3,1],\n",
    "                                       'Single match':agg_data.iloc[0,1],'No match':agg_data.iloc[2,1],\n",
    "                                       'Concordant':agg_data.iloc[1,1]}, ignore_index=True)\n",
    "\n",
    "for f in listdir(data): \n",
    "    \n",
    "    if '_concordant_freqTable.csv' in f:\n",
    "        freq = pd.read_csv(join(data,f), sep='\\t')\n",
    "        sample_name = f.split('_')[0]\n",
    "        for index, row in graph_data.iterrows():\n",
    "            if graph_data.iloc[index,0] == sample_name:\n",
    "                for f_index, f_row in freq.iterrows():\n",
    "                    primerpair =  freq.iloc[f_index,0] + ', ' + freq.iloc[f_index, 1]\n",
    "                    graph_data.loc[index, primerpair] = freq.iloc[f_index, 2]        \n",
    "\n",
    "## Optional: Add total columns to calculate primer totals to graph_data df for column and row \n",
    "## (if you only uncomment this part w/o modification, will mess up total values for calculating avg_rep)\n",
    "\n",
    "##graph_data.loc['Column Total'] = graph_data.sum(numeric_only=True, axis = 0, skipna = True) \n",
    "##graph_data.loc[:,'Primer Count Total'] = graph_data.sum(numeric_only=True, axis = 1, skipna = True)\n",
    "\n",
    "############################################################################\n",
    "## Calculate frequency for average primer representation dataframe based on first 20 std cond experiments\n",
    "\n",
    "std_cond = graph_data.head(20).copy()\n",
    "\n",
    "std_cond.loc['Total'] = std_cond.sum(numeric_only=True, axis = 0, skipna = True) \n",
    "std_cond.loc[:,'Primer Count Total'] = std_cond.sum(numeric_only=True, axis = 1, skipna = True)\n",
    "\n",
    "totalcount = std_cond.iloc[len(std_cond)-1, len(std_cond.columns)-1]\n",
    "\n",
    "counter = 0\n",
    "for i in range(6, len(std_cond.columns)-1):\n",
    "    occurence = std_cond.iloc[len(std_cond)-1,i]\n",
    "    frequency = (occurence/totalcount)*100\n",
    "    avg_rep.iloc[counter, 4] = round(frequency, 3)\n",
    "    counter += 1\n",
    "\n",
    "############################################################################\n",
    "## Load both dataframes into csv files\n",
    "\n",
    "graph_data_csv = join(data, 'graphable_data.csv')\n",
    "graph_data.to_csv(graph_data_csv, sep='\\t', index=False)\n",
    "\n",
    "avg_rep_csv = join(data, 'avg_primer_representation.csv')\n",
    "avg_rep.to_csv(avg_rep_csv, sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Using online blast to generate custom db\n",
    "\n",
    "Everything below this is me trying to figure out how to use BLAST through Biopython through online BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test to try to blast a sequence over internet\n",
    "\n",
    "from Bio.Blast import NCBIWWW\n",
    "from Bio import SeqIO\n",
    "\n",
    "result_handle = NCBIWWW.qblast(\"blastn\",\"nt\", \"CTGGGACCCGACAGTTGTCAT\")\n",
    "with open(\"my_blast.xml\", \"w\") as out_handle:\n",
    "    out_handle.write(result_handle.read())\n",
    "    \n",
    "result_handle.close()\n",
    "result_handle = open(\"my_blast.xml\")\n",
    "\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "blast_record = NCBIXML.read(result_handle)\n",
    "\n",
    "E_VALUE_THRESH = 0.5\n",
    "\n",
    "for alignment in blast_record.alignments:\n",
    "    for hsp in alignment.hsps:\n",
    "        if hsp.expect < E_VALUE_THRESH:\n",
    "            print(\"sequence:\", alignment.title)\n",
    "            print(\"length:\", alignment.length)\n",
    "            print(\"e value:\", hsp.expect)\n",
    "            print(hsp.query[0:75] + \"...\")\n",
    "            print(hsp.match[0:75] + \"...\")\n",
    "            print(hsp.sbjct[0:75] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: tested the commands through terminal\n",
    "<br>** In order to run short sequences of alignment, have to use -word_size 7 or else the code will say there are no alignments even when there are\n",
    "<br>** Making a blast database (Makeblastdb) is easiest done through terminal, but actual local blast can be through jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test to make a database from a downloaded genome off flybase\n",
    "\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "file = '/Users/annechen/Downloads/dmel-all-chromosome-r5.49.fasta'\n",
    "\n",
    "## Command Line args for local blast\n",
    "\n",
    "## makeblastdb -in dmel-all-chromosome-r5.49.fasta -dbtype nucl\n",
    "## blastn -query testprimerf.fasta -db dmel-all-chromosome-r5.49.fasta -evalue 0.1 -word_size 7\n",
    "\n",
    "blastx_cline = NcbiblastxCommandline(cmd='blastn', query='PCRProject/query.fasta', db='PCRProject/dmel-all-chromosome-r5.49.fasta', evalue=0.1, out='PCRProject/TestBlast.xml', outfmt=5, word_size=7)\n",
    "\n",
    "blastx_cline()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a fasta file of all primers given a avg_rep.csv (generated from cell 5)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "avg_rep_file = '/Users/annechen/Downloads/CRISPR Pools Data/results/avg_primer_representation.csv'\n",
    "\n",
    "avg_rep = pd.read_csv(avg_rep_file, sep = '\\t')\n",
    "\n",
    "primerf = open(\"PCRProject/primerf.fasta\", 'w')\n",
    "for index, row in avg_rep.iterrows():\n",
    "    primerf.write('>'+avg_rep.iloc[index,0]+' row='+str(index)+'\\n'+avg_rep.iloc[index,2]+'\\n')\n",
    "    primerf.write('>'+avg_rep.iloc[index,1]+' row='+str(index)+'\\n'+avg_rep.iloc[index,3]+'\\n')\n",
    "\n",
    "primerf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BLAST Fasta file\n",
    "\n",
    "##In terminal: to make db from fasta\n",
    "##makeblastdb -in dmel-all-chromosome-r5.49.fasta -dbtype nucl\n",
    "\n",
    "\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "primerf = 'PCRProject/primerf.fasta'\n",
    "dmeldb = 'PCRProject/dmel-all-aligned-r5.49.fasta'\n",
    "\n",
    "blastx_cline = NcbiblastxCommandline(cmd='blastn', query=primerf, db=dmeldb, evalue=0.5, out='PCRProject/TestBlast.xml', outfmt=5)\n",
    "\n",
    "blastx_cline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primerID</th>\n",
       "      <th>rowID</th>\n",
       "      <th>hitID</th>\n",
       "      <th>locStart</th>\n",
       "      <th>locEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JH441</td>\n",
       "      <td>0</td>\n",
       "      <td>gi|669632474|ref|NC_004354.4|</td>\n",
       "      <td>759135</td>\n",
       "      <td>759155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JH471</td>\n",
       "      <td>0</td>\n",
       "      <td>gi|669632474|ref|NC_004354.4|</td>\n",
       "      <td>759384</td>\n",
       "      <td>759362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JH442</td>\n",
       "      <td>1</td>\n",
       "      <td>gi|669632474|ref|NC_004354.4|</td>\n",
       "      <td>764953</td>\n",
       "      <td>764975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH472</td>\n",
       "      <td>1</td>\n",
       "      <td>gi|669632474|ref|NC_004354.4|</td>\n",
       "      <td>765211</td>\n",
       "      <td>765192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JH443</td>\n",
       "      <td>2</td>\n",
       "      <td>gi|669632474|ref|NC_004354.4|</td>\n",
       "      <td>796293</td>\n",
       "      <td>796318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primerID rowID                          hitID locStart  locEnd\n",
       "0    JH441     0  gi|669632474|ref|NC_004354.4|   759135  759155\n",
       "1    JH471     0  gi|669632474|ref|NC_004354.4|   759384  759362\n",
       "2    JH442     1  gi|669632474|ref|NC_004354.4|   764953  764975\n",
       "3    JH472     1  gi|669632474|ref|NC_004354.4|   765211  765192\n",
       "4    JH443     2  gi|669632474|ref|NC_004354.4|   796293  796318"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a df of the blast xml results file\n",
    "## This cell ends up not being used (use below cell to create a dictionary instead)\n",
    "\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "avg_rep_file = '/Users/annechen/Downloads/CRISPR Pools Data/results/avg_primer_representation.csv'\n",
    "blastdf = pd.DataFrame(columns = ['primerID', 'rowID', 'hitID','locStart','locEnd'])\n",
    "\n",
    "result_handle = open(\"PCRProject/9T6PZX5C016-Alignment.xml\")\n",
    "##result_handle = open(\"PCRProject/Test.xml\")\n",
    "\n",
    "blast_records = NCBIXML.parse(result_handle)\n",
    "\n",
    "for blast_record in blast_records:\n",
    "    for alignment in blast_record.alignments:\n",
    "        for hsp in alignment.hsps:\n",
    "            if hsp.expect< 0.02:\n",
    "                ID = blast_record.query\n",
    "                blastdf = blastdf.append({'primerID':ID.split('r')[0],'rowID':ID.split('=')[1],'hitID':alignment.hit_id,\n",
    "                                         'locStart':hsp.sbjct_start, 'locEnd':hsp.sbjct_end}, ignore_index=True)\n",
    "                break\n",
    "\n",
    "                \n",
    "blastdf.head()\n",
    "\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'24': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1922815, 'end': 1922834, 'primerID': u'JH465'}, {'start': 1923088, 'end': 1923068, 'primerID': u'JH495'}]}, u'25': {u'gi|669632474|ref|NC_004354.4|': [{'start': 2041719, 'end': 2041740, 'primerID': u'JH466'}, {'start': 2041983, 'end': 2041964, 'primerID': u'JH496'}]}, u'26': {u'gi|669632474|ref|NC_004354.4|': [{'start': 2080418, 'end': 2080437, 'primerID': u'JH467'}, {'start': 2080716, 'end': 2080696, 'primerID': u'JH497'}]}, u'27': {u'gi|669632474|ref|NC_004354.4|': [{'start': 2270729, 'end': 2270749, 'primerID': u'JH468'}, {'start': 2271027, 'end': 2271008, 'primerID': u'JH498'}]}, u'20': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1669560, 'end': 1669583, 'primerID': u'JH461'}, {'start': 1669894, 'end': 1669874, 'primerID': u'JH491'}]}, u'21': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1695701, 'end': 1695722, 'primerID': u'JH462'}, {'start': 1696003, 'end': 1695980, 'primerID': u'JH492'}]}, u'22': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1889784, 'end': 1889804, 'primerID': u'JH463'}, {'start': 1890035, 'end': 1890014, 'primerID': u'JH493'}]}, u'23': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1908316, 'end': 1908335, 'primerID': u'JH464'}, {'start': 1908586, 'end': 1908566, 'primerID': u'JH494'}]}, u'28': {u'gi|669632474|ref|NC_004354.4|': [{'start': 2340169, 'end': 2340190, 'primerID': u'JH469'}, {'start': 2340451, 'end': 2340430, 'primerID': u'JH499'}]}, u'29': {u'gi|669632474|ref|NC_004354.4|': [{'start': 2448243, 'end': 2448265, 'primerID': u'JH470'}, {'start': 2448514, 'end': 2448493, 'primerID': u'JH500'}]}, u'1': {u'gi|669632474|ref|NC_004354.4|': [{'start': 764953, 'end': 764975, 'primerID': u'JH442'}, {'start': 765211, 'end': 765192, 'primerID': u'JH472'}]}, u'0': {u'gi|669632474|ref|NC_004354.4|': [{'start': 759135, 'end': 759155, 'primerID': u'JH441'}, {'start': 759384, 'end': 759362, 'primerID': u'JH471'}]}, u'3': {u'gi|669632474|ref|NC_004354.4|': [{'start': 801264, 'end': 801283, 'primerID': u'JH444'}, {'start': 801530, 'end': 801511, 'primerID': u'JH474'}]}, u'2': {u'gi|669632474|ref|NC_004354.4|': [{'start': 796293, 'end': 796318, 'primerID': u'JH443'}, {'start': 796587, 'end': 796568, 'primerID': u'JH473'}]}, u'5': {u'gi|669632474|ref|NC_004354.4|': [{'start': 934882, 'end': 934903, 'primerID': u'JH446'}, {'start': 935163, 'end': 935142, 'primerID': u'JH476'}]}, u'4': {u'gi|669632474|ref|NC_004354.4|': [{'start': 805929, 'end': 805952, 'primerID': u'JH445'}, {'start': 806216, 'end': 806197, 'primerID': u'JH475'}]}, u'7': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1209740, 'end': 1209762, 'primerID': u'JH448'}, {'start': 1210016, 'end': 1209996, 'primerID': u'JH478'}]}, u'6': {u'gi|669632474|ref|NC_004354.4|': [{'start': 940706, 'end': 940726, 'primerID': u'JH447'}, {'start': 940955, 'end': 940936, 'primerID': u'JH477'}]}, u'9': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1286307, 'end': 1286326, 'primerID': u'JH450'}, {'start': 1286558, 'end': 1286539, 'primerID': u'JH480'}]}, u'8': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1236325, 'end': 1236349, 'primerID': u'JH449'}, {'start': 1236574, 'end': 1236555, 'primerID': u'JH479'}]}, u'11': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1353945, 'end': 1353964, 'primerID': u'JH452'}, {'start': 1354265, 'end': 1354246, 'primerID': u'JH482'}]}, u'10': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1288936, 'end': 1288958, 'primerID': u'JH451'}, {'start': 1289217, 'end': 1289196, 'primerID': u'JH481'}]}, u'13': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1375394, 'end': 1375415, 'primerID': u'JH454'}, {'start': 1375645, 'end': 1375627, 'primerID': u'JH484'}]}, u'12': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1370523, 'end': 1370542, 'primerID': u'JH453'}, {'start': 1370798, 'end': 1370778, 'primerID': u'JH483'}]}, u'15': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1399693, 'end': 1399712, 'primerID': u'JH456'}, {'start': 1400032, 'end': 1400013, 'primerID': u'JH486'}]}, u'14': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1392896, 'end': 1392919, 'primerID': u'JH455'}, {'start': 1393152, 'end': 1393132, 'primerID': u'JH485'}]}, u'17': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1478345, 'end': 1478367, 'primerID': u'JH458'}, {'start': 1478661, 'end': 1478642, 'primerID': u'JH488'}]}, u'16': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1477099, 'end': 1477118, 'primerID': u'JH457'}, {'start': 1477363, 'end': 1477343, 'primerID': u'JH487'}]}, u'19': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1512847, 'end': 1512867, 'primerID': u'JH460'}, {'start': 1513175, 'end': 1513153, 'primerID': u'JH490'}]}, u'18': {u'gi|669632474|ref|NC_004354.4|': [{'start': 1483264, 'end': 1483284, 'primerID': u'JH459'}, {'start': 1483544, 'end': 1483525, 'primerID': u'JH489'}]}}\n"
     ]
    }
   ],
   "source": [
    "## Create a nested dict of data in the blast xml results file \n",
    "## Basically does same thing as above cell but through a different method to make easier data retrieval\n",
    "\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "avg_rep_file = '/Users/annechen/Downloads/CRISPR Pools Data/results/avg_primer_representation.csv'\n",
    "blastdf = pd.DataFrame(columns = ['primerID', 'rowID', 'hitID','locStart','locEnd'])\n",
    "blast_result_dict = {}\n",
    "\n",
    "result_handle = open(\"PCRProject/9T6PZX5C016-Alignment.xml\")\n",
    "##result_handle = open(\"PCRProject/Test.xml\")\n",
    "\n",
    "## Create initial nested dict of data from blast xml file\n",
    "\n",
    "blast_records = NCBIXML.parse(result_handle)\n",
    "for blast_record in blast_records:\n",
    "    for alignment in blast_record.alignments:\n",
    "        for hsp in alignment.hsps:\n",
    "            if hsp.expect< 0.02:\n",
    "                ID = blast_record.query\n",
    "                hitId = alignment.hit_id\n",
    "                rowId = ID.split('=')[1]\n",
    "                rowRec = blast_result_dict.get(rowId) \n",
    "                if rowRec == None:\n",
    "                    rowRec = {}\n",
    "                    blast_result_dict[rowId] = rowRec\n",
    "                hitList = rowRec.get(hitId)\n",
    "                if hitList == None:\n",
    "                    hitList = []\n",
    "                    rowRec[hitId] = hitList\n",
    "                hitEntry = {}\n",
    "                hitEntry['primerID'] =  ID.split('r')[0]\n",
    "                hitEntry['start'] = hsp.sbjct_start\n",
    "                hitEntry['end'] = hsp.sbjct_end\n",
    "                hitList.append (hitEntry)\n",
    "\n",
    "                break\n",
    "\n",
    "                \n",
    "## Loop through dictionary to throw out chromosome repeats: isolate matching primer sequences\n",
    "\n",
    "for row_num in blast_result_dict:\n",
    "    hits = blast_result_dict[row_num]\n",
    "    tobedeleted= []\n",
    "    for locdict in hits:\n",
    "        hitList = hits[locdict]\n",
    "        if len(hitList) != 2:\n",
    "            tobedeleted.append(locdict)\n",
    "    for i in tobedeleted:\n",
    "        del hits[i]\n",
    "\n",
    "## blast_result_dict holds a nested dictionary of all the information\n",
    "##print blast_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forPrimerID</th>\n",
       "      <th>revPrimerID</th>\n",
       "      <th>hitID</th>\n",
       "      <th>rowID</th>\n",
       "      <th>locStart</th>\n",
       "      <th>locEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JH441</td>\n",
       "      <td>JH471</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>0</td>\n",
       "      <td>759135</td>\n",
       "      <td>759384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JH442</td>\n",
       "      <td>JH472</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>1</td>\n",
       "      <td>764953</td>\n",
       "      <td>765211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JH443</td>\n",
       "      <td>JH473</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>2</td>\n",
       "      <td>796293</td>\n",
       "      <td>796587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH444</td>\n",
       "      <td>JH474</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>3</td>\n",
       "      <td>801264</td>\n",
       "      <td>801530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JH445</td>\n",
       "      <td>JH475</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>4</td>\n",
       "      <td>805929</td>\n",
       "      <td>806216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JH446</td>\n",
       "      <td>JH476</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>5</td>\n",
       "      <td>934882</td>\n",
       "      <td>935163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JH447</td>\n",
       "      <td>JH477</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>6</td>\n",
       "      <td>940706</td>\n",
       "      <td>940955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JH448</td>\n",
       "      <td>JH478</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>7</td>\n",
       "      <td>1209740</td>\n",
       "      <td>1210016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JH449</td>\n",
       "      <td>JH479</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>8</td>\n",
       "      <td>1236325</td>\n",
       "      <td>1236574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JH450</td>\n",
       "      <td>JH480</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>9</td>\n",
       "      <td>1286307</td>\n",
       "      <td>1286558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JH451</td>\n",
       "      <td>JH481</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>10</td>\n",
       "      <td>1288936</td>\n",
       "      <td>1289217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JH452</td>\n",
       "      <td>JH482</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>11</td>\n",
       "      <td>1353945</td>\n",
       "      <td>1354265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JH453</td>\n",
       "      <td>JH483</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>12</td>\n",
       "      <td>1370523</td>\n",
       "      <td>1370798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JH454</td>\n",
       "      <td>JH484</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>13</td>\n",
       "      <td>1375394</td>\n",
       "      <td>1375645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JH455</td>\n",
       "      <td>JH485</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>14</td>\n",
       "      <td>1392896</td>\n",
       "      <td>1393152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JH456</td>\n",
       "      <td>JH486</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>15</td>\n",
       "      <td>1399693</td>\n",
       "      <td>1400032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JH457</td>\n",
       "      <td>JH487</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>16</td>\n",
       "      <td>1477099</td>\n",
       "      <td>1477363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JH458</td>\n",
       "      <td>JH488</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>17</td>\n",
       "      <td>1478345</td>\n",
       "      <td>1478661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JH459</td>\n",
       "      <td>JH489</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>18</td>\n",
       "      <td>1483264</td>\n",
       "      <td>1483544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JH460</td>\n",
       "      <td>JH490</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>19</td>\n",
       "      <td>1512847</td>\n",
       "      <td>1513175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JH461</td>\n",
       "      <td>JH491</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>20</td>\n",
       "      <td>1669560</td>\n",
       "      <td>1669894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JH462</td>\n",
       "      <td>JH492</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>21</td>\n",
       "      <td>1695701</td>\n",
       "      <td>1696003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>JH463</td>\n",
       "      <td>JH493</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>22</td>\n",
       "      <td>1889784</td>\n",
       "      <td>1890035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JH464</td>\n",
       "      <td>JH494</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>23</td>\n",
       "      <td>1908316</td>\n",
       "      <td>1908586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JH465</td>\n",
       "      <td>JH495</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>24</td>\n",
       "      <td>1922815</td>\n",
       "      <td>1923088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JH466</td>\n",
       "      <td>JH496</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>25</td>\n",
       "      <td>2041719</td>\n",
       "      <td>2041983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>JH467</td>\n",
       "      <td>JH497</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>26</td>\n",
       "      <td>2080418</td>\n",
       "      <td>2080716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JH468</td>\n",
       "      <td>JH498</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>27</td>\n",
       "      <td>2270729</td>\n",
       "      <td>2271027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JH469</td>\n",
       "      <td>JH499</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>28</td>\n",
       "      <td>2340169</td>\n",
       "      <td>2340451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JH470</td>\n",
       "      <td>JH500</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>29</td>\n",
       "      <td>2448243</td>\n",
       "      <td>2448514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   forPrimerID revPrimerID      hitID rowID locStart   locEnd\n",
       "0        JH441       JH471  NC_004354     0   759135   759384\n",
       "1        JH442       JH472  NC_004354     1   764953   765211\n",
       "2        JH443       JH473  NC_004354     2   796293   796587\n",
       "3        JH444       JH474  NC_004354     3   801264   801530\n",
       "4        JH445       JH475  NC_004354     4   805929   806216\n",
       "5        JH446       JH476  NC_004354     5   934882   935163\n",
       "6        JH447       JH477  NC_004354     6   940706   940955\n",
       "7        JH448       JH478  NC_004354     7  1209740  1210016\n",
       "8        JH449       JH479  NC_004354     8  1236325  1236574\n",
       "9        JH450       JH480  NC_004354     9  1286307  1286558\n",
       "10       JH451       JH481  NC_004354    10  1288936  1289217\n",
       "11       JH452       JH482  NC_004354    11  1353945  1354265\n",
       "12       JH453       JH483  NC_004354    12  1370523  1370798\n",
       "13       JH454       JH484  NC_004354    13  1375394  1375645\n",
       "14       JH455       JH485  NC_004354    14  1392896  1393152\n",
       "15       JH456       JH486  NC_004354    15  1399693  1400032\n",
       "16       JH457       JH487  NC_004354    16  1477099  1477363\n",
       "17       JH458       JH488  NC_004354    17  1478345  1478661\n",
       "18       JH459       JH489  NC_004354    18  1483264  1483544\n",
       "19       JH460       JH490  NC_004354    19  1512847  1513175\n",
       "20       JH461       JH491  NC_004354    20  1669560  1669894\n",
       "21       JH462       JH492  NC_004354    21  1695701  1696003\n",
       "22       JH463       JH493  NC_004354    22  1889784  1890035\n",
       "23       JH464       JH494  NC_004354    23  1908316  1908586\n",
       "24       JH465       JH495  NC_004354    24  1922815  1923088\n",
       "25       JH466       JH496  NC_004354    25  2041719  2041983\n",
       "26       JH467       JH497  NC_004354    26  2080418  2080716\n",
       "27       JH468       JH498  NC_004354    27  2270729  2271027\n",
       "28       JH469       JH499  NC_004354    28  2340169  2340451\n",
       "29       JH470       JH500  NC_004354    29  2448243  2448514"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "blastdbfinal = pd.DataFrame(columns = ['forPrimerID', 'revPrimerID', 'hitID', 'rowID', 'locStart','locEnd'])\n",
    "\n",
    "##min = 9999999999\n",
    "##max = 0\n",
    "count = 0\n",
    "for row_num in blast_result_dict:\n",
    "    hits = blast_result_dict[row_num]\n",
    "    for locdict in hits:\n",
    "        min = 9999999999\n",
    "        max = 0\n",
    "        hitList = hits[locdict]\n",
    "        p1 = hitList[0]['primerID']\n",
    "        p2= hitList[1]['primerID']\n",
    "        \n",
    "        m = re.search('ref\\|(.*)\\..+\\|',locdict)\n",
    "        refseqid = m.group(1)\n",
    "        blastdbfinal = blastdbfinal.append({'rowID':int(row_num),'hitID':refseqid, 'forPrimerID':p1,\n",
    "                                    'revPrimerID':p2}, ignore_index=True)\n",
    "\n",
    "        for read in hitList:\n",
    "            #read = hitList[i]\n",
    "            if read['start']<min:\n",
    "                min = read['start']\n",
    "            if read['start']>max:\n",
    "                max = read['start']\n",
    "            if read['end']<min:\n",
    "                min = read['end']\n",
    "            if read['end']>max:\n",
    "                max = read['end']\n",
    "        blastdbfinal.iloc[count,4]=min\n",
    "        blastdbfinal.iloc[count,5]=max\n",
    "        count += 1\n",
    "\n",
    "blastdbfinal = blastdbfinal.sort_values('rowID', ascending=True)\n",
    "blastdbfinal = blastdbfinal.reset_index()\n",
    "del blastdbfinal['index']\n",
    "\n",
    "blastdbfinal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([u'NC_004354'])\n"
     ]
    }
   ],
   "source": [
    "## Takes in a downloaded file of dmel chromosome data and creates an output fasta file of the custom db\n",
    "## This was a test cell that ISN'T ULTIMATELY USED: the dmel file I downloaded didn't align quite right\n",
    "    ## with the code --> had to instead download the individual chromosome file off ncbi blast website\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from Bio.Blast import NCBIXML\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sets import Set\n",
    "\n",
    "dmelfasta = 'PCRProject/dmel-all-chromosome-r5.49.fasta'\n",
    "\n",
    "uniqueRefSeqIds = Set()\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    uniqueRefSeqIds.add(blastdbfinal.iloc[index,2])\n",
    "    \n",
    "print uniqueRefSeqIds\n",
    "\n",
    "fastaSeq = {}\n",
    "\n",
    "\n",
    "with open(dmelfasta, 'rU') as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "       ## print record.description\n",
    "        m = re.search('(REFSEQ\\:(.*)\\,GB:\\S+);',record.description)\n",
    "        if m:\n",
    "            ids= m.group(1)\n",
    "            refSeqID = m.group(2)\n",
    "            ##print refSeqID\n",
    "            ##print ids\n",
    "            if refSeqID in uniqueRefSeqIds:\n",
    "                fastaInfo = {}\n",
    "                fastaInfo['id'] = record.id\n",
    "                fastaInfo['dbxref'] = ids\n",
    "                fastaInfo['seq'] = str(record.seq)\n",
    "                fastaSeq[refSeqID] = fastaInfo\n",
    "\n",
    "##print fastaSeq\n",
    "f = open('PCRProject/customdb2.fasta', 'w')\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    working = fastaSeq[blastdbfinal.iloc[index,2]]\n",
    "    start = blastdbfinal.iloc[index,4]\n",
    "    end = blastdbfinal.iloc[index,5]\n",
    "    f.write('>'+working['id']+' loc='+str(start)+'..'+str(end)+' dbxref='+working['dbxref']+' fwdPrimer='\n",
    "            +blastdbfinal.iloc[index,0]+' revPrimer='+blastdbfinal.iloc[index,1]+'\\n')\n",
    "    f.write(working['seq'][start-1: end-1]+'\\n')\n",
    "f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([u'NC_004354'])\n"
     ]
    }
   ],
   "source": [
    "## Takes in a fasta file (downloaded directly off NCBI as a separate file for each chromosome)\n",
    "## Creates custom db in fasta format\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from Bio.Blast import NCBIXML\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sets import Set\n",
    "\n",
    "dmelchromfasta = 'PCRProject/sequence.fasta'\n",
    "\n",
    "## Create set of seqref IDs (catalogues chromosome number)\n",
    "## Based on results of set, download corresponding chromosome numbers of NCBI website\n",
    "\n",
    "uniqueRefSeqIds = Set()\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    uniqueRefSeqIds.add(blastdbfinal.iloc[index,2])\n",
    "    \n",
    "print uniqueRefSeqIds\n",
    "\n",
    "## Parses downloaded fasta file of chromosome and pulls out relevant info to put in dictionary\n",
    "\n",
    "fastaSeq = {}\n",
    "\n",
    "with open(dmelchromfasta, 'rU') as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        m = re.search('(.*)\\..+',str(record.id))\n",
    "        refSeqID = m.group(1)\n",
    "        if refSeqID in uniqueRefSeqIds:\n",
    "            fastaInfo = {}\n",
    "            fastaInfo['id'] = refSeqID\n",
    "            fastaInfo['header'] = record.description\n",
    "            fastaInfo['seq'] = str(record.seq)\n",
    "            fastaSeq[refSeqID] = fastaInfo\n",
    "        ##print record.id\n",
    "        ##print record.description\n",
    "\n",
    "## Creates a new fasta file to write in the custom db and save it        \n",
    "\n",
    "f = open('PCRProject/customdb.fasta', 'w')\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    working = fastaSeq[blastdbfinal.iloc[index,2]]\n",
    "    start = blastdbfinal.iloc[index,4]\n",
    "    end = blastdbfinal.iloc[index,5]\n",
    "    f.write('>'+working['header']+' loc='+str(start)+'..'+str(end)+' fwdPrimer='\n",
    "            +blastdbfinal.iloc[index,0]+' revPrimer='+blastdbfinal.iloc[index,1]+'\\n')\n",
    "    f.write(working['seq'][start-1: end]+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final: Put Everything Above Together and Use Local Blast!\n",
    "Basically modified code to use local blast after I figured out the word_size thing (without it my code wasn't working so I did everything with online blast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################3\n",
    "## Make a fasta file called primerf.fasta for all the primers to run through local blast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "avg_rep_file = '/Users/annechen/Downloads/CRISPR Pools Data/results/avg_primer_representation.csv'\n",
    "\n",
    "avg_rep = pd.read_csv(avg_rep_file, sep = '\\t')\n",
    "\n",
    "primerf = open(\"PCRProject/primerf.fasta\", 'w')\n",
    "for index, row in avg_rep.iterrows():\n",
    "    primerf.write('>'+avg_rep.iloc[index,0]+' row='+str(index)+'\\n'+avg_rep.iloc[index,2]+'\\n')\n",
    "    primerf.write('>'+avg_rep.iloc[index,1]+' row='+str(index)+'\\n'+avg_rep.iloc[index,3]+'\\n')\n",
    "\n",
    "primerf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local blast against primerf.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a db from an input dmel genome file\n",
    "\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "file = 'PCRProject/dmel-all-chromosome-r5.49.fasta'\n",
    "\n",
    "#################################################################################\n",
    "## First use linux for creating db from dmel genome I downloaded off flybase\n",
    "\n",
    "## makeblastdb -in dmel-all-chromosome-r5.49.fasta -dbtype nucl\n",
    "\n",
    "#################################################################################\n",
    "## Run a local blast and output an xml file to parse the results and primer location\n",
    "\n",
    "## Can be done either through terminal: \n",
    "## blastn -query testprimerf.fasta -db dmel-all-chromosome-r5.49.fasta -evalue 0.1 -word_size 7\n",
    "\n",
    "## or through jupyter notebook:\n",
    "blastx_cline = NcbiblastxCommandline(cmd='blastn', query='PCRProject/primerf.fasta', db='PCRProject/dmel-all-chromosome-r5.49.fasta', \n",
    "                                     evalue=0.1, out='PCRProject/BlastResults.xml', outfmt=5, word_size=7)\n",
    "\n",
    "blastx_cline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Local Blast XML File Results \n",
    "First cell: Input Blast Results XML File data into a nested dictionary. Throw out any reads on different chromosomes that do not match\n",
    "<br>Second cell: Determines start and end locations of each primer and inputs final data into a dataframe (blastdbfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a nested dict of data in the blast xml results file \n",
    "\n",
    "from Bio.Blast import NCBIXML\n",
    "\n",
    "blastdf = pd.DataFrame(columns = ['primerID', 'rowID', 'hitID','locStart','locEnd'])\n",
    "blast_result_dict = {}\n",
    "\n",
    "result_handle = open(\"PCRProject/BlastResults.xml\")\n",
    "\n",
    "#################################################################################\n",
    "## Create initial nested dict of data from blast xml file\n",
    "\n",
    "blast_records = NCBIXML.parse(result_handle)\n",
    "for blast_record in blast_records:\n",
    "    for alignment in blast_record.alignments:\n",
    "        for hsp in alignment.hsps:\n",
    "            if hsp.expect< 0.02:\n",
    "                ID = blast_record.query\n",
    "                m = re.search('REFSEQ\\:(.*)\\,GB:\\S+;',alignment.hit_def)\n",
    "                hitId = m.group(1)\n",
    "                rowId = ID.split('=')[1]\n",
    "                rowRec = blast_result_dict.get(rowId) \n",
    "                if rowRec == None:\n",
    "                    rowRec = {}\n",
    "                    blast_result_dict[rowId] = rowRec\n",
    "                hitList = rowRec.get(hitId)\n",
    "                if hitList == None:\n",
    "                    hitList = []\n",
    "                    rowRec[hitId] = hitList\n",
    "                hitEntry = {}\n",
    "                hitEntry['primerID'] =  ID.split('r')[0]\n",
    "                hitEntry['start'] = hsp.sbjct_start\n",
    "                hitEntry['end'] = hsp.sbjct_end\n",
    "                hitList.append (hitEntry)\n",
    "\n",
    "                break\n",
    "\n",
    "                \n",
    "## Loop through dictionary to throw out chromosome repeats: isolate matching primer sequences\n",
    "\n",
    "for row_num in blast_result_dict:\n",
    "    hits = blast_result_dict[row_num]\n",
    "    tobedeleted= []\n",
    "    for locdict in hits:\n",
    "        hitList = hits[locdict]\n",
    "        if len(hitList) != 2:\n",
    "            tobedeleted.append(locdict)\n",
    "    for i in tobedeleted:\n",
    "        del hits[i]\n",
    "\n",
    "## blast_result_dict holds a nested dictionary of all the information\n",
    "## print blast_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forPrimerID</th>\n",
       "      <th>revPrimerID</th>\n",
       "      <th>hitID</th>\n",
       "      <th>rowID</th>\n",
       "      <th>locStart</th>\n",
       "      <th>locEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JH441</td>\n",
       "      <td>JH471</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>0</td>\n",
       "      <td>653168</td>\n",
       "      <td>653417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JH442</td>\n",
       "      <td>JH472</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>1</td>\n",
       "      <td>658986</td>\n",
       "      <td>659244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JH443</td>\n",
       "      <td>JH473</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>2</td>\n",
       "      <td>690326</td>\n",
       "      <td>690620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH444</td>\n",
       "      <td>JH474</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>3</td>\n",
       "      <td>695297</td>\n",
       "      <td>695563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JH445</td>\n",
       "      <td>JH475</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>4</td>\n",
       "      <td>699962</td>\n",
       "      <td>700249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JH446</td>\n",
       "      <td>JH476</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>5</td>\n",
       "      <td>828915</td>\n",
       "      <td>829196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JH447</td>\n",
       "      <td>JH477</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>6</td>\n",
       "      <td>834739</td>\n",
       "      <td>834988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JH448</td>\n",
       "      <td>JH478</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>7</td>\n",
       "      <td>1103773</td>\n",
       "      <td>1104049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JH449</td>\n",
       "      <td>JH479</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>8</td>\n",
       "      <td>1130358</td>\n",
       "      <td>1130607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JH450</td>\n",
       "      <td>JH480</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>9</td>\n",
       "      <td>1180340</td>\n",
       "      <td>1180591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JH451</td>\n",
       "      <td>JH481</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>10</td>\n",
       "      <td>1182969</td>\n",
       "      <td>1183250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JH452</td>\n",
       "      <td>JH482</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>11</td>\n",
       "      <td>1247978</td>\n",
       "      <td>1248298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JH453</td>\n",
       "      <td>JH483</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>12</td>\n",
       "      <td>1264556</td>\n",
       "      <td>1264831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JH454</td>\n",
       "      <td>JH484</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>13</td>\n",
       "      <td>1269427</td>\n",
       "      <td>1269678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JH455</td>\n",
       "      <td>JH485</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>14</td>\n",
       "      <td>1286929</td>\n",
       "      <td>1287185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JH456</td>\n",
       "      <td>JH486</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>15</td>\n",
       "      <td>1293726</td>\n",
       "      <td>1294065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JH457</td>\n",
       "      <td>JH487</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>16</td>\n",
       "      <td>1371132</td>\n",
       "      <td>1371396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JH458</td>\n",
       "      <td>JH488</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>17</td>\n",
       "      <td>1372378</td>\n",
       "      <td>1372694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JH459</td>\n",
       "      <td>JH489</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>18</td>\n",
       "      <td>1377297</td>\n",
       "      <td>1377577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JH460</td>\n",
       "      <td>JH490</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>19</td>\n",
       "      <td>1406880</td>\n",
       "      <td>1407208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JH461</td>\n",
       "      <td>JH491</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>20</td>\n",
       "      <td>1563593</td>\n",
       "      <td>1563927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JH462</td>\n",
       "      <td>JH492</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>21</td>\n",
       "      <td>1589734</td>\n",
       "      <td>1590036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>JH463</td>\n",
       "      <td>JH493</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>22</td>\n",
       "      <td>1783817</td>\n",
       "      <td>1784068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JH464</td>\n",
       "      <td>JH494</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>23</td>\n",
       "      <td>1802349</td>\n",
       "      <td>1802619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JH465</td>\n",
       "      <td>JH495</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>24</td>\n",
       "      <td>1816848</td>\n",
       "      <td>1817121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JH466</td>\n",
       "      <td>JH496</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>25</td>\n",
       "      <td>1935752</td>\n",
       "      <td>1936016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>JH467</td>\n",
       "      <td>JH497</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>26</td>\n",
       "      <td>1974451</td>\n",
       "      <td>1974749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JH468</td>\n",
       "      <td>JH498</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>27</td>\n",
       "      <td>2164762</td>\n",
       "      <td>2165060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JH469</td>\n",
       "      <td>JH499</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>28</td>\n",
       "      <td>2234202</td>\n",
       "      <td>2234484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JH470</td>\n",
       "      <td>JH500</td>\n",
       "      <td>NC_004354</td>\n",
       "      <td>29</td>\n",
       "      <td>2342276</td>\n",
       "      <td>2342547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   forPrimerID revPrimerID      hitID rowID locStart   locEnd\n",
       "0        JH441       JH471  NC_004354     0   653168   653417\n",
       "1        JH442       JH472  NC_004354     1   658986   659244\n",
       "2        JH443       JH473  NC_004354     2   690326   690620\n",
       "3        JH444       JH474  NC_004354     3   695297   695563\n",
       "4        JH445       JH475  NC_004354     4   699962   700249\n",
       "5        JH446       JH476  NC_004354     5   828915   829196\n",
       "6        JH447       JH477  NC_004354     6   834739   834988\n",
       "7        JH448       JH478  NC_004354     7  1103773  1104049\n",
       "8        JH449       JH479  NC_004354     8  1130358  1130607\n",
       "9        JH450       JH480  NC_004354     9  1180340  1180591\n",
       "10       JH451       JH481  NC_004354    10  1182969  1183250\n",
       "11       JH452       JH482  NC_004354    11  1247978  1248298\n",
       "12       JH453       JH483  NC_004354    12  1264556  1264831\n",
       "13       JH454       JH484  NC_004354    13  1269427  1269678\n",
       "14       JH455       JH485  NC_004354    14  1286929  1287185\n",
       "15       JH456       JH486  NC_004354    15  1293726  1294065\n",
       "16       JH457       JH487  NC_004354    16  1371132  1371396\n",
       "17       JH458       JH488  NC_004354    17  1372378  1372694\n",
       "18       JH459       JH489  NC_004354    18  1377297  1377577\n",
       "19       JH460       JH490  NC_004354    19  1406880  1407208\n",
       "20       JH461       JH491  NC_004354    20  1563593  1563927\n",
       "21       JH462       JH492  NC_004354    21  1589734  1590036\n",
       "22       JH463       JH493  NC_004354    22  1783817  1784068\n",
       "23       JH464       JH494  NC_004354    23  1802349  1802619\n",
       "24       JH465       JH495  NC_004354    24  1816848  1817121\n",
       "25       JH466       JH496  NC_004354    25  1935752  1936016\n",
       "26       JH467       JH497  NC_004354    26  1974451  1974749\n",
       "27       JH468       JH498  NC_004354    27  2164762  2165060\n",
       "28       JH469       JH499  NC_004354    28  2234202  2234484\n",
       "29       JH470       JH500  NC_004354    29  2342276  2342547"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Takes in nested dictionary from previous cell and converts it to a readable dataframe\n",
    "import re\n",
    "\n",
    "blastdbfinal = pd.DataFrame(columns = ['forPrimerID', 'revPrimerID', 'hitID', 'rowID', 'locStart','locEnd'])\n",
    "\n",
    "##min = 9999999999\n",
    "##max = 0\n",
    "count = 0\n",
    "for row_num in blast_result_dict:\n",
    "    hits = blast_result_dict[row_num]\n",
    "    for locdict in hits:\n",
    "        min = 9999999999\n",
    "        max = 0\n",
    "        hitList = hits[locdict]\n",
    "        p1 = hitList[0]['primerID']\n",
    "        p2= hitList[1]['primerID']\n",
    "        blastdbfinal = blastdbfinal.append({'rowID':int(row_num),'hitID':refseqid, 'forPrimerID':p1,\n",
    "                                    'revPrimerID':p2}, ignore_index=True)\n",
    "        for read in hitList:\n",
    "            #read = hitList[i]\n",
    "            if read['start']<min:\n",
    "                min = read['start']\n",
    "            if read['start']>max:\n",
    "                max = read['start']\n",
    "            if read['end']<min:\n",
    "                min = read['end']\n",
    "            if read['end']>max:\n",
    "                max = read['end']\n",
    "        blastdbfinal.iloc[count,4]=min\n",
    "        blastdbfinal.iloc[count,5]=max\n",
    "        count += 1\n",
    "\n",
    "blastdbfinal = blastdbfinal.sort_values('rowID', ascending=True)\n",
    "blastdbfinal = blastdbfinal.reset_index()\n",
    "del blastdbfinal['index']\n",
    "\n",
    "## blastdbfinal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches XML file results to create custom fasta db\n",
    "Don't forget to use terminal to convert fasta to db at very end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes in a downloaded file of dmel chromosome data and creates an output fasta file of the custom db\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from Bio.Blast import NCBIXML\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sets import Set\n",
    "\n",
    "dmelfasta = 'PCRProject/dmel-all-chromosome-r5.49.fasta'\n",
    "\n",
    "uniqueRefSeqIds = Set()\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    uniqueRefSeqIds.add(blastdbfinal.iloc[index,2])\n",
    "    \n",
    "fastaSeq = {}\n",
    "\n",
    "\n",
    "with open(dmelfasta, 'rU') as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        m = re.search('(REFSEQ\\:(.*)\\,GB:\\S+);',record.description)\n",
    "        if m:\n",
    "            ids= m.group(1)\n",
    "            refSeqID = m.group(2)\n",
    "            if refSeqID in uniqueRefSeqIds:\n",
    "                fastaInfo = {}\n",
    "                fastaInfo['id'] = record.id\n",
    "                fastaInfo['dbxref'] = ids\n",
    "                fastaInfo['seq'] = str(record.seq)\n",
    "                fastaSeq[refSeqID] = fastaInfo\n",
    "\n",
    "##print fastaSeq\n",
    "f = open('PCRProject/customdb2.fasta', 'w')\n",
    "for index, row in blastdbfinal.iterrows():\n",
    "    working = fastaSeq[blastdbfinal.iloc[index,2]]\n",
    "    start = blastdbfinal.iloc[index,4]\n",
    "    end = blastdbfinal.iloc[index,5]\n",
    "    f.write('>'+working['id']+'_'+str(index)+' loc='+str(start)+'..'+str(end)+' dbxref='+working['dbxref']+' fwdPrimer='\n",
    "            +blastdbfinal.iloc[index,0]+' revPrimer='+blastdbfinal.iloc[index,1]+'\\n')\n",
    "    f.write(working['seq'][start-1: end]+'\\n')\n",
    "f.close()\n",
    "    \n",
    "## After creating file, use terminal to convert to db:\n",
    "## makeblastdb -in customdb2.fasta -dbtype nucl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
